{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce0c7341-fc00-4ff9-9468-8cc8185cd24a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as func\n",
    "from datetime import datetime\n",
    "from pyspark.sql import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6fa5070-2fc0-441e-8a3f-3860cbfea1b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account = 'test1fast'\n",
    "access_key = 'QnSkjChqVUQWCLs9t+yDSK4w02oQVBjWtP9dOOBhpw1O002GrWnk8LHfsU8Ys16QjNKmjnDw2RbM+AStEQNjww=='\n",
    "spark.conf.set(\n",
    "  f\"fs.azure.account.key.{storage_account}.blob.core.windows.net\",\n",
    "  f\"{access_key}\"\n",
    ")\n",
    "\n",
    "container_name = 'streaming-events-captured'\n",
    "\n",
    "try:\n",
    "    dbutils.fs.mount(\n",
    "    source=f\"wasbs://{container_name}@{storage_account}.blob.core.windows.net\",\n",
    "    mount_point=\"/mnt/blob_storage\",\n",
    "    extra_configs={\n",
    "        f\"fs.azure.account.key.{storage_account}.blob.core.windows.net\": f\"{access_key}\"\n",
    "    }\n",
    "    )\n",
    "except:\n",
    "    pass\n",
    "\n",
    "list_files_to_ingest = dbutils.fs.ls(\"/mnt/blob_storage\")\n",
    "all_json_files = [some_file.path for some_file in list_files_to_ingest]\n",
    "reception_time = int(datetime.utcnow().timestamp())\n",
    "df = spark.read.json(all_json_files)\\\n",
    "                .withColumn(\"source_file\", func.input_file_name())\\\n",
    "                .withColumn(\"unixReviewTime\", func.lit(reception_time)\n",
    "                            )\n",
    "\n",
    "max_id = spark.sql('SELECT MAX(ID) AS MAX_ID FROM reviews').toPandas()['MAX_ID'][0]\n",
    "window_spec = Window.orderBy('partition_number')\n",
    "df = df.withColumn(\"ID\", func.row_number().over(window_spec) + max_id + 1)\n",
    "\n",
    "df.select('asin',\n",
    " 'image',\n",
    " 'overall',\n",
    " 'reviewText',\n",
    " 'reviewerID',\n",
    " 'reviewerName',\n",
    " 'style',\n",
    " 'summary',\n",
    " 'unixReviewTime',\n",
    " 'verified',\n",
    " 'vote',\n",
    " 'ID').dropDuplicates().write.format(\"delta\").mode(\"append\").insertInto(\"reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c29985d-5e6e-4270-af7f-2122c8507006",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c8b5a5a-5f33-4ae7-90b7-c691b38a1aa5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18e63672-d68e-4517-afd1-122fd9bb9ad3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d6e3c83-a2c8-48f8-b177-5048e5c28f23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3727885097642882,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "moving_batch_streaming_files",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
